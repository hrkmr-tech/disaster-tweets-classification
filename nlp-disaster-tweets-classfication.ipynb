{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Disaster Tweets Kaggle Mini-Project\n\nThe purpose of this project is to predict whether there is a disaster or not from the provided tweets in the dataset.\n\nThe dataset has 7,613 tweets in total. Each of the tweets has the columns or features `id`, `keyword`, `location`, `text` and `target`. The column `target` has two values: `0` for 'Non-Disaster' and `1` for 'Disaster'.\n\n## Summary\n\nOnly `text` and `target` are used for training models. After EDA and data cleansing, we created a CNN model and a multi-channel CNN model in this project.\n\n**the validation accuracy**\n\n- the CNN model: around 80%\n- the multi-channel CNN model: around 80%\n\nBoth of the CNN model and the multi-channel CNN model are as good: *the CNN model (the simpler one) is selected for our final model*.","metadata":{"id":"ptkHaiID_EWG","_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-10T18:17:05.827889Z","iopub.execute_input":"2021-06-10T18:17:05.828417Z","iopub.status.idle":"2021-06-10T18:17:40.26371Z","shell.execute_reply.started":"2021-06-10T18:17:05.82828Z","shell.execute_reply":"2021-06-10T18:17:40.262629Z"}}},{"cell_type":"markdown","source":"##  Table of Contents\n\n1. Importing Libraries and Dataset\n\n2. Exploring Data\n\n3. Cleansing Data\n\n4. Analyzing Data\n\n5. Preprocessing Data\n\n6. Training Models\n\n7. Discussion","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Libraries and Dataset","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries\n\nFirst, we will import the libraries to be used in this project.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import *\nfrom keras.layers import *\nfrom keras.models import Sequential, Model\nimport kerastuner as kt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"ptkHaiID_EWG","_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-15T08:58:37.010818Z","iopub.execute_input":"2022-06-15T08:58:37.011182Z","iopub.status.idle":"2022-06-15T08:58:43.279701Z","shell.execute_reply.started":"2022-06-15T08:58:37.011103Z","shell.execute_reply":"2022-06-15T08:58:43.278736Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Importing the Data\n\nSecond, we will import tha train dataset.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"id":"maaZD2m7GrYZ","execution":{"iopub.status.busy":"2022-06-15T08:58:43.283374Z","iopub.execute_input":"2022-06-15T08:58:43.283620Z","iopub.status.idle":"2022-06-15T08:58:43.322097Z","shell.execute_reply.started":"2022-06-15T08:58:43.283596Z","shell.execute_reply":"2022-06-15T08:58:43.321339Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Showing the 10 rows in the dataset.","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"id":"NluCZIHYHDQU","outputId":"5e7f9692-a804-44e3-f2bf-f91d8de296a4","execution":{"iopub.status.busy":"2022-06-15T08:58:43.323869Z","iopub.execute_input":"2022-06-15T08:58:43.324216Z","iopub.status.idle":"2022-06-15T08:58:43.346653Z","shell.execute_reply.started":"2022-06-15T08:58:43.324180Z","shell.execute_reply":"2022-06-15T08:58:43.345641Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Showing the brief summary of the dataset.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"id":"j7WlTIr9HE9a","outputId":"efaeacad-dcbf-479b-fe93-feb5ba103831","execution":{"iopub.status.busy":"2022-06-15T08:58:43.348269Z","iopub.execute_input":"2022-06-15T08:58:43.348624Z","iopub.status.idle":"2022-06-15T08:58:43.366620Z","shell.execute_reply.started":"2022-06-15T08:58:43.348590Z","shell.execute_reply":"2022-06-15T08:58:43.365519Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The column `id` have to be removed because it can influence the result even though it is just random numbers.\nWe also remove the columns `keyword` and `location` as they have a lot of`null` values and are of little use.\n\nWe only need the columns `text` and `target` in this project.","metadata":{}},{"cell_type":"code","source":"data = data[['text','target']]\ndata.head()","metadata":{"id":"pYXi7Ltic6iG","outputId":"a0ba0fd6-5162-4946-87b6-e07d4932dce1","execution":{"iopub.status.busy":"2022-06-15T08:58:43.367891Z","iopub.execute_input":"2022-06-15T08:58:43.368415Z","iopub.status.idle":"2022-06-15T08:58:43.379399Z","shell.execute_reply.started":"2022-06-15T08:58:43.368379Z","shell.execute_reply":"2022-06-15T08:58:43.378415Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 2. Exploring Data","metadata":{}},{"cell_type":"markdown","source":"### Showing the Number of Non-Disaster and Disaster Tweets\n\nWe will show the number of the categories (Non-Disaster and Disaster).\n\nThe number of Non-Disaster tweets is 1,000 more than that of Disaster ones. But this is not a large problem because the number of tweets in both of the categories is large enough.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot(x='target', data=data)\n\nax.set(xlabel='Tweets (0=Non-Disaster, 1=Disaster)')\nplt.show()","metadata":{"id":"K-Ly_g_lHShG","execution":{"iopub.status.busy":"2022-06-15T08:58:43.380976Z","iopub.execute_input":"2022-06-15T08:58:43.381335Z","iopub.status.idle":"2022-06-15T08:58:43.505745Z","shell.execute_reply.started":"2022-06-15T08:58:43.381302Z","shell.execute_reply":"2022-06-15T08:58:43.504705Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Showing the Length of Tweets\n\nWe will show the length of tweets in both of the categories.","metadata":{}},{"cell_type":"code","source":"len_of_non_disaster = data[data['target']==0]['text'].map(lambda x : len(x))\nlen_of_disaster = data[data['target']==1]['text'].map(lambda x : len(x))\n\nlen_data = {\n    'len_of_non_disaster': len_of_non_disaster,\n    'len_of_disaster': len_of_disaster\n}\n\n# non_disaster\nplt.figure(figsize=(10,8))\nax = sns.histplot(data=len_data, x='len_of_non_disaster')\nax.set(xlabel='Length of Non-Disaster Tweets')\nplt.show()\n# disaster\nplt.figure(figsize=(10,8))\nax = sns.histplot(data=len_data, x='len_of_disaster')\nax.set(xlabel='Length of Disaster Tweets')\nplt.show()","metadata":{"id":"rph6u8Ewjxra","execution":{"iopub.status.busy":"2022-06-15T08:58:43.507133Z","iopub.execute_input":"2022-06-15T08:58:43.507536Z","iopub.status.idle":"2022-06-15T08:58:43.904824Z","shell.execute_reply.started":"2022-06-15T08:58:43.507499Z","shell.execute_reply":"2022-06-15T08:58:43.903964Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we can say this:\n- the number ratio of Disaster tweets between the length of 100 and 130 is smaller than that of Non-Disaster ones\n- the number ratio of Disaster tweets more than the length of 140 is larger than that of Non-Disaster ones","metadata":{}},{"cell_type":"markdown","source":"#### Comparing the Mean and Median\n\nWe will compare the mean and median in the length of tweets.","metadata":{}},{"cell_type":"code","source":"print('the length of Non-Disaster tweets. mean={}, median={}'.format(np.mean(len_of_non_disaster), np.median(len_of_non_disaster)))\nprint('the length of Disaster tweets. mean={}, median={}'.format(np.mean(len_of_disaster), np.median(len_of_disaster)))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:58:43.907427Z","iopub.execute_input":"2022-06-15T08:58:43.907963Z","iopub.status.idle":"2022-06-15T08:58:43.915776Z","shell.execute_reply.started":"2022-06-15T08:58:43.907920Z","shell.execute_reply":"2022-06-15T08:58:43.914881Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We can say from the result\n- the length of Disaster tweets is longer than that of Non-Disaster tweets in both mean and median","metadata":{}},{"cell_type":"markdown","source":"### Showing the Length of Words in Tweets\n\nNow, we will check the length of words in tweets.","metadata":{}},{"cell_type":"code","source":"len_of_words_non_disaster = data[data['target']==0]['text'].str.split().map(lambda x : len(x))\nlen_of_words_disaster = data[data['target']==1]['text'].str.split().map(lambda x : len(x))\n\nlen_words_data = {\n    'len_of_words_non_disaster': len_of_words_non_disaster,\n    'len_of_words_disaster': len_of_words_disaster\n}\n\n# non_disaster\nplt.figure(figsize=(10,8))\nax = sns.histplot(data=len_words_data, x='len_of_words_non_disaster')\nax.set(xlabel='Length of Words in Non-Disaster Tweets')\nplt.show()\n# disaster\nplt.figure(figsize=(10,8))\nax = sns.histplot(data=len_words_data, x='len_of_words_disaster')\nax.set(xlabel='Length of Words in Disaster Tweets')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:58:43.917970Z","iopub.execute_input":"2022-06-15T08:58:43.918429Z","iopub.status.idle":"2022-06-15T08:58:44.306876Z","shell.execute_reply.started":"2022-06-15T08:58:43.918389Z","shell.execute_reply":"2022-06-15T08:58:44.306043Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We can say from the plot:\n- the length of words is almost the same","metadata":{}},{"cell_type":"markdown","source":"#### Comparing the Mean and Median\n\nWe will compare the mean and median in the length of words in tweets.","metadata":{}},{"cell_type":"code","source":"print('the length of words in Non-Disaster tweets. mean={}, median={}'.format(np.mean(len_of_words_non_disaster), np.median(len_of_words_non_disaster)))\nprint('the length of words in Disaster tweets. mean={}, median={}'.format(np.mean(len_of_words_disaster), np.median(len_of_words_disaster)))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:58:44.308172Z","iopub.execute_input":"2022-06-15T08:58:44.308542Z","iopub.status.idle":"2022-06-15T08:58:44.316031Z","shell.execute_reply.started":"2022-06-15T08:58:44.308504Z","shell.execute_reply":"2022-06-15T08:58:44.314907Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We can say from the result:\n- The mean and median in the length of words in tweets are almost the same in both of the categories","metadata":{}},{"cell_type":"markdown","source":"## 3. Cleansing Data\n\nIn this section, we will apply the cleansing methods below:\n\n- removing url\n- removing html tags\n- removing digits\n- removing punctuations\n- lowering all letters\n- removing stopwords\n- lemmatizing\n- removing 1 length words\n\nIf we remove 'noisy' words and symbols here, the models created on the cleansed data is better performed than not.","metadata":{}},{"cell_type":"code","source":"def preprocess_data(text):\n    # removing url\n    text = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',text)\n    \n    # removing html tags\n    text = re.sub(r'<.*?>',' ',text)\n    \n    # removing degits\n    text = re.sub(r'[0-9]', '', text)\n    \n    # removeing mentions\n    text = re.sub('@\\S+', '', text)\n    \n    # removing punctuations (see: https://stackoverflow.com/a/37221663)\n    table = str.maketrans(dict.fromkeys(string.punctuation))\n    text = text.translate(table) \n    \n    # lowering all letters\n    text = text.lower()\n    text = text.split()\n    \n    # removing stopwords and lemmatizing\n    lemmatizer = WordNetLemmatizer()\n    text = [lemmatizer.lemmatize(words) for words in text if words not in stopwords.words('english')]\n    \n    # removing 1 length words\n    text = [i for i in text if len(i)>=2] \n    text = ' '.join(text)\n    return text\n\n# apply cleansing\ndata['cleansed'] = data['text'].apply(preprocess_data)","metadata":{"id":"sckzCfiLegfL","execution":{"iopub.status.busy":"2022-06-15T08:58:44.317381Z","iopub.execute_input":"2022-06-15T08:58:44.317941Z","iopub.status.idle":"2022-06-15T08:58:58.344324Z","shell.execute_reply.started":"2022-06-15T08:58:44.317903Z","shell.execute_reply":"2022-06-15T08:58:58.343454Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Showing the cleansed data.","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"id":"iwX5VY5bK13S","outputId":"7ecbefb1-5c8c-4e22-aebb-8264be851a15","execution":{"iopub.status.busy":"2022-06-15T08:58:58.345527Z","iopub.execute_input":"2022-06-15T08:58:58.345882Z","iopub.status.idle":"2022-06-15T08:58:58.356827Z","shell.execute_reply.started":"2022-06-15T08:58:58.345846Z","shell.execute_reply":"2022-06-15T08:58:58.355904Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 4. Analyzing Data","metadata":{}},{"cell_type":"markdown","source":"### Creating the WordCloud","metadata":{}},{"cell_type":"markdown","source":"Creating the WordCloud using the library (https://www.python-graph-gallery.com/wordcloud/).","metadata":{}},{"cell_type":"code","source":"def wordcloud(data,title):\n    words = ' '.join(data['cleansed'].astype('str').tolist())\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(stopwords = stopwords, width= 512, height = 512).generate(words)\n    plt.figure(figsize=(10,8),frameon=True)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.title(title,fontsize=20)\n    plt.show()\n\ndata_disaster = data[data['target'] == 1]\ndata_non_disaster = data[data['target'] == 0]","metadata":{"id":"rA860VD7K5zQ","execution":{"iopub.status.busy":"2022-06-15T08:58:58.358343Z","iopub.execute_input":"2022-06-15T08:58:58.358880Z","iopub.status.idle":"2022-06-15T08:58:58.369960Z","shell.execute_reply.started":"2022-06-15T08:58:58.358844Z","shell.execute_reply":"2022-06-15T08:58:58.369205Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Non-Disaster Tweets WordCloud\n\nWe can detect the words `love` , `want`, `lol`, `time` and so on, which might not be be associated with a disaster.","metadata":{}},{"cell_type":"code","source":"wordcloud(data_non_disaster,'Non-Disaster Tweets')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:58:58.371434Z","iopub.execute_input":"2022-06-15T08:58:58.372833Z","iopub.status.idle":"2022-06-15T08:58:59.564148Z","shell.execute_reply.started":"2022-06-15T08:58:58.372782Z","shell.execute_reply":"2022-06-15T08:58:59.563219Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### Disaster Tweets WordCloud\n\nWe can detect the words`flood`, `storm`, `fire`, `police` and so on, which might be associated with a disaster. ","metadata":{}},{"cell_type":"code","source":"wordcloud(data_disaster,'Disaster Tweets')","metadata":{"id":"zH60osphMhQ0","outputId":"9f045705-2762-4571-ebf5-3e946c14e2a4","execution":{"iopub.status.busy":"2022-06-15T08:58:59.565219Z","iopub.execute_input":"2022-06-15T08:58:59.565601Z","iopub.status.idle":"2022-06-15T08:59:01.091938Z","shell.execute_reply.started":"2022-06-15T08:58:59.565560Z","shell.execute_reply":"2022-06-15T08:59:01.091149Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Showing top 10 unigrams and bigrams","metadata":{}},{"cell_type":"code","source":"def top_ngrams(data,n,grams):\n    count_vec = CountVectorizer(ngram_range=(grams,grams)).fit(data)\n    bow = count_vec.transform(data)\n    add_words = bow.sum(axis=0)\n    word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True) \n    return word_freq[:n]","metadata":{"id":"7FrdJlJNQGCa","execution":{"iopub.status.busy":"2022-06-15T08:59:01.093461Z","iopub.execute_input":"2022-06-15T08:59:01.094083Z","iopub.status.idle":"2022-06-15T08:59:01.101207Z","shell.execute_reply.started":"2022-06-15T08:59:01.094041Z","shell.execute_reply":"2022-06-15T08:59:01.100170Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"common_uni_df = pd.DataFrame(top_ngrams(data['cleansed'], 10, 1), columns=['word','freq'])\ncommon_bi_df = pd.DataFrame(top_ngrams(data['cleansed'], 10, 2), columns=['word','freq'])\n\nplt.figure(figsize=(10,8))\nsns.barplot(x='word', y='freq', data=common_uni_df)\nplt.xticks(rotation=90)\nplt.show()\n\nplt.figure(figsize=(10,8))\nsns.barplot(x='word', y='freq', data=common_bi_df)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"id":"BmAP6cJhXQGU","execution":{"iopub.status.busy":"2022-06-15T08:59:01.102540Z","iopub.execute_input":"2022-06-15T08:59:01.102865Z","iopub.status.idle":"2022-06-15T08:59:02.336103Z","shell.execute_reply.started":"2022-06-15T08:59:01.102831Z","shell.execute_reply":"2022-06-15T08:59:02.335305Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## 5. Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"### Extracting the feature and label","metadata":{}},{"cell_type":"code","source":"X = data['cleansed']\ny = data['target']","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:59:02.338230Z","iopub.execute_input":"2022-06-15T08:59:02.338858Z","iopub.status.idle":"2022-06-15T08:59:02.342952Z","shell.execute_reply.started":"2022-06-15T08:59:02.338818Z","shell.execute_reply":"2022-06-15T08:59:02.342205Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Tokenizing the Texts\n\nTokenizing the texts. Tokenizing will split the texts into the vectors of words so that the computer can process them.","metadata":{}},{"cell_type":"code","source":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(X.values)\nvocab_length = len(word_tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:59:02.345991Z","iopub.execute_input":"2022-06-15T08:59:02.346328Z","iopub.status.idle":"2022-06-15T08:59:02.474448Z","shell.execute_reply.started":"2022-06-15T08:59:02.346293Z","shell.execute_reply":"2022-06-15T08:59:02.473654Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Padding the Text\n\nPadding the text so that each text has the same length.","metadata":{}},{"cell_type":"code","source":"longest_train = max(X.values, key=lambda text: len(word_tokenize(text)))\nlength_long_text = len(word_tokenize(longest_train))\n\n# padding\npadded_texts = pad_sequences(word_tokenizer.texts_to_sequences(X.values), length_long_text, padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:59:02.476046Z","iopub.execute_input":"2022-06-15T08:59:02.476393Z","iopub.status.idle":"2022-06-15T08:59:03.712897Z","shell.execute_reply.started":"2022-06-15T08:59:02.476359Z","shell.execute_reply":"2022-06-15T08:59:03.712062Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Embedding Word Matrix\n\nWe will create our own word matrix which return the vector of word (Vector Data Source: https://www.kaggle.com/danielwillgeorge/glove6b100dtxt)","metadata":{}},{"cell_type":"code","source":"embeddings_dictionary = dict()\nembedding_dim = 100\n\n\nTXT_PATH = '../input/glove6b100dtxt/glove.6B.100d.txt'\n\n# loading the vectors of words from the file\nwith open(TXT_PATH) as f:\n    for line in f:\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary[word] = vector_dimensions\n\n# create matrix\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:59:03.714110Z","iopub.execute_input":"2022-06-15T08:59:03.714473Z","iopub.status.idle":"2022-06-15T08:59:20.496875Z","shell.execute_reply.started":"2022-06-15T08:59:03.714438Z","shell.execute_reply":"2022-06-15T08:59:20.496040Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## 6. Training Models\n\nWe will create two models in this section: a convolution neural network model and a multi-channel convolution neural network model.","metadata":{}},{"cell_type":"markdown","source":"### Spliting the Data for Train and Validation\nWe will split the data at the ratio below:\n- train_size: 80% of the data\n- validaton_size: 20% of the data","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(padded_texts, y.values, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:59:20.498022Z","iopub.execute_input":"2022-06-15T08:59:20.498361Z","iopub.status.idle":"2022-06-15T08:59:20.505637Z","shell.execute_reply.started":"2022-06-15T08:59:20.498327Z","shell.execute_reply":"2022-06-15T08:59:20.504729Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# a helper function for showing model history\ndef model_history(model_history):\n    epochs = list(range(1, len(model_history.history['accuracy']) + 1))\n    # accuracy    \n    data_preproc = pd.DataFrame({\n        'epochs': epochs, \n        'accuracy': model_history.history['accuracy'],\n        'val_accuracy': model_history.history['val_accuracy']\n    })\n    plt.figure(figsize=(8,6))\n    ax = sns.lineplot(x='epochs', y='value', hue='variable', \n             data=pd.melt(data_preproc, ['epochs']))\n    ax.set_title('model accuracy')\n    plt.show()\n\n    # loss    \n    data_preproc = pd.DataFrame({\n        'epochs': epochs, \n        'loss': model_history.history['loss'],\n        'val_loss': model_history.history['val_loss']\n    })\n    plt.figure(figsize=(8,6))\n    ax = sns.lineplot(x='epochs', y='value', hue='variable', \n             data=pd.melt(data_preproc, ['epochs']))\n    ax.set_title('model loss')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T08:59:20.506979Z","iopub.execute_input":"2022-06-15T08:59:20.507340Z","iopub.status.idle":"2022-06-15T08:59:20.517966Z","shell.execute_reply.started":"2022-06-15T08:59:20.507305Z","shell.execute_reply":"2022-06-15T08:59:20.517104Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Convolutional Neural Network\n\nWe will create a normal CNN model (single input) with the definition of the next code.","metadata":{}},{"cell_type":"code","source":"def create_cnn_model(hp):    \n    model = Sequential()\n\n    model.add(Embedding(vocab_length, 100, weights=[embedding_matrix],\n                                     input_length=length_long_text,trainable=False))\n    model.add(Conv1D(filters=hp.Int('conv_1_filter',min_value=21,max_value=200,step=14),\n                                kernel_size=hp.Choice('conv_1_kernel',values=[3,4,5]),\n                                activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(units = hp.Int('dense_1',min_value=21,max_value=150,step=14), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1,activation='sigmoid'))\n  \n    hp_learning_rate = hp.Choice('learning_rate', values=[3e-2, 3e-3, 3e-4, 3e-5])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n    return model","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-15T08:59:20.519348Z","iopub.execute_input":"2022-06-15T08:59:20.519637Z","iopub.status.idle":"2022-06-15T08:59:20.530002Z","shell.execute_reply.started":"2022-06-15T08:59:20.519611Z","shell.execute_reply":"2022-06-15T08:59:20.528817Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Using Keras-Tuner(kt) to get the best hyperparameters.","metadata":{}},{"cell_type":"code","source":"tuner_cnn = kt.Hyperband(create_cnn_model,\n                         objective='val_accuracy',\n                         max_epochs=15,factor=5,\n                         directory='cnn_model',\n                         project_name='DisasterTweets',\n                         overwrite=True)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-15T08:59:20.531634Z","iopub.execute_input":"2022-06-15T08:59:20.531972Z","iopub.status.idle":"2022-06-15T08:59:23.102508Z","shell.execute_reply.started":"2022-06-15T08:59:20.531939Z","shell.execute_reply":"2022-06-15T08:59:23.101657Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"stop_early = EarlyStopping(monitor='val_loss',\n                           mode='min',\n                           verbose=1,\n                           patience=10)\n\ntuner_cnn.search(X_train, y_train,\n                 epochs=15,\n                 validation_data=(X_val,y_val),\n                 callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hyperparams_cnn = tuner_cnn.get_best_hyperparameters(num_trials=1)[0]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-15T08:59:23.103757Z","iopub.execute_input":"2022-06-15T08:59:23.104109Z","iopub.status.idle":"2022-06-15T09:00:41.084069Z","shell.execute_reply.started":"2022-06-15T08:59:23.104073Z","shell.execute_reply":"2022-06-15T09:00:41.083245Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Creating a model using the best hyperparameters.","metadata":{}},{"cell_type":"code","source":"model_cnn = tuner_cnn.hypermodel.build(best_hyperparams_cnn)\n# showing the model summary\nmodel_cnn.summary()","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-15T09:00:41.085550Z","iopub.execute_input":"2022-06-15T09:00:41.085983Z","iopub.status.idle":"2022-06-15T09:00:41.379879Z","shell.execute_reply.started":"2022-06-15T09:00:41.085935Z","shell.execute_reply":"2022-06-15T09:00:41.379055Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# train\nhistory_cnn = model_cnn.fit(X_train,\n                            y_train,\n                            epochs=50,\n                            validation_data=(X_val,y_val),\n                            callbacks=[stop_early])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:41.381063Z","iopub.execute_input":"2022-06-15T09:00:41.381414Z","iopub.status.idle":"2022-06-15T09:00:51.307292Z","shell.execute_reply.started":"2022-06-15T09:00:41.381376Z","shell.execute_reply":"2022-06-15T09:00:51.306232Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Plotting the accuracy and loss history. **The validation accuracy is around 80% in the 2nd epoch**.","metadata":{}},{"cell_type":"code","source":"model_history(history_cnn)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:51.308714Z","iopub.execute_input":"2022-06-15T09:00:51.309042Z","iopub.status.idle":"2022-06-15T09:00:51.941588Z","shell.execute_reply.started":"2022-06-15T09:00:51.309005Z","shell.execute_reply":"2022-06-15T09:00:51.940427Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Multi-Channel Convolutional Neural Network\n\nWe will create a normal CNN model (double inputs) with the definition of the next code.","metadata":{}},{"cell_type":"code","source":"def create_multi_channel_cnn(hp):\n    # input 1\n    inputs1 = Input(shape=(length_long_text,))\n\n    embedding1 = Embedding(vocab_length, 100, weights=[embedding_matrix],\n                           input_length=length_long_text, trainable=False)(inputs1)     \n    conv1 = Conv1D(filters=hp.Int('conv_1_filter',min_value=21,max_value=150,step=14),\n                                kernel_size=hp.Choice('conv_1_kernel',values=[3,4,5,6,7,8]),\n                                activation='relu')(embedding1) \n    drop1 = Dropout(0.3)(conv1) \n    pool1 = MaxPooling1D()(drop1) \n    flat1 = Flatten()(pool1)\n    \n    # input 2\n    inputs2 = Input(shape=(length_long_text,)) \n    embedding2 = Embedding(vocab_length, 100, weights=[embedding_matrix],\n                           input_length=length_long_text,trainable=False)(inputs2) \n    conv2 = Conv1D(filters=hp.Int('conv_2_filter',min_value=21,max_value=150,step=14),\n                                kernel_size=hp.Choice('conv_2_kernel',values=[3,4,5,6,7,8]),\n                                activation='relu')(embedding2) \n    drop2 = Dropout(0.3)(conv2) \n    pool2 = MaxPooling1D()(drop2) \n    flat2 = Flatten()(pool2) \n    \n    # merge \n    merged = concatenate([flat1, flat2]) \n    \n    dense1 = Dense(units = hp.Int('dense_1',min_value=21,max_value=120,step=14),\n                               activation='relu')(merged)\n    drop3 = Dropout(0.5)(dense1)\n    \n    outputs = Dense(1, activation='sigmoid')(drop3) \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs) \n    \n    hp_learning_rate = hp.Choice('learning_rate', values=[3e-2, 3e-3, 3e-4, 3e-5]) \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n    return model","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-15T09:00:51.945364Z","iopub.execute_input":"2022-06-15T09:00:51.945713Z","iopub.status.idle":"2022-06-15T09:00:51.966575Z","shell.execute_reply.started":"2022-06-15T09:00:51.945677Z","shell.execute_reply":"2022-06-15T09:00:51.965548Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Using Keras-Tuner(kt) to get the best hyperparameters.","metadata":{}},{"cell_type":"code","source":"tuner_mcnn = kt.Hyperband(create_multi_channel_cnn,\n                          objective='val_accuracy',\n                          max_epochs=15,factor=5,\n                          directory='my_dir',\n                          project_name='DisasterTweetsMCNN_kt',\n                          overwrite=True)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-15T09:00:51.968722Z","iopub.execute_input":"2022-06-15T09:00:51.969076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_early = EarlyStopping(monitor='val_loss',\n                           mode='min',\n                           verbose=1,\n                           patience=10)\n\ntuner_mcnn.search([X_train,X_train],y_train, epochs=15,\n                  validation_data=([X_val,X_val], y_val),\n                  callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hyperparams_mcnn = tuner_mcnn.get_best_hyperparameters(num_trials=1)[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a model using the best hyperparameters.","metadata":{}},{"cell_type":"code","source":"model_mcnn = tuner_mcnn.hypermodel.build(best_hyperparams_mcnn)\n# showing the model summary\nmodel_mcnn.summary()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_mcnn = model_mcnn.fit([X_train,X_train], y_train,epochs=50,\n                              validation_data=([X_val,X_val], y_val),\n                              callbacks=[stop_early])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the accuracy and loss history. **The validation accuracy is around 80% in the 1st epoch**.","metadata":{}},{"cell_type":"code","source":"model_history(history_mcnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion\n\nBoth of the models have almost the same validation accuracy (around 80%) even though the number of inputs is different. Some causes might be:\n\n- the quality of the provided data (raw data) is not high enough at first\n- the data cleansing is not enough to remove 'noisy' words and symbols\n- the model definition is not sophisticated because of my poor ability\n- some of the methods including data manipulation and modeling to be used is not suitable for the task\n\nIn the next trial, we'd like to aim at 90% with a completely different approach.","metadata":{}},{"cell_type":"markdown","source":"### Kaggle Submission","metadata":{}},{"cell_type":"code","source":"# importaing test data\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\ntest[\"cleansed\"] = test[\"text\"].apply(preprocess_data)\ntest_sentences = pad_sequences(word_tokenizer.texts_to_sequences(test[\"cleansed\"].values),\n                               length_long_text,\n                               padding='post')","metadata":{"execution":{"iopub.status.idle":"2022-06-15T09:02:42.527126Z","shell.execute_reply.started":"2022-06-15T09:02:36.992715Z","shell.execute_reply":"2022-06-15T09:02:42.526328Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# using the CNN model\npreds = model_cnn.predict_classes(test_sentences)\n\nsubmission = pd.concat([\n    pd.DataFrame(test[\"id\"]),\n    pd.DataFrame(preds)\n], axis=1)\nsubmission.columns = [\"id\",\"target\"]\n\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:02:42.528356Z","iopub.execute_input":"2022-06-15T09:02:42.528715Z","iopub.status.idle":"2022-06-15T09:02:43.024529Z","shell.execute_reply.started":"2022-06-15T09:02:42.528679Z","shell.execute_reply":"2022-06-15T09:02:43.023682Z"},"trusted":true},"execution_count":36,"outputs":[]}]}